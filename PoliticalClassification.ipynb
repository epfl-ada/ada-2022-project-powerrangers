{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: typing_extensions in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from seaborn) (4.2.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ewais/anaconda3/envs/deca-env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # setting ignore as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [03:28,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from reduced files\n",
    "#df_ch_f = pd.read_pickle('data/df_channels_reduced.tsv.gz')\n",
    "#df_sb_f = pd.read_pickle('data/df_timeseries_reduced.tsv.gz')\n",
    "\n",
    "#df_sb_f[\"datetime\"] = pd.to_datetime(df_sb_f[\"datetime\"])\n",
    "#df_ch_f[\"join_date\"] = pd.to_datetime(df_ch_f[\"join_date\"])\n",
    "columns = ['categories', 'channel_id', 'crawl_date', 'description',\n",
    "       'dislike_count', 'display_id', 'duration', 'like_count', 'tags',\n",
    "       'title', 'upload_date', 'view_count']\n",
    "vd_frames = []\n",
    "for chunk in tqdm(pd.read_csv('data/yt_metadata_reduced.tsv.gz', chunksize=10**5, names=columns,lineterminator='\\n')):\n",
    "       chunk[\"dummmy\"] = 1\n",
    "       vd_frames.append(chunk)\n",
    "df_vd_f = pd.concat(vd_frames)\n",
    "df_vd_f = df_vd_f[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vd_f[\"description\"].astype(str)\n",
    "\n",
    "test_set = df_vd_f[df_vd_f[\"categories\"] == \"News & Politics\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_vd_f[\"description\"].values.tolist()\n",
    "\n",
    "test_text = []\n",
    "for sent in test_set: \n",
    "    test_text.append(str(sent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_fwf(\"data/keyword-tweets.txt\")\n",
    "\n",
    "train_df = pd.read_csv('data/keyword-tweets.txt', delimiter = \"\\t\")\n",
    "\n",
    "train_df.columns = [\"label\", \"Tweet\"]\n",
    "\n",
    "train_df = train_df.applymap(str)\n",
    "\n",
    "train_df.label = (train_df[\"label\"] == \"POLIT\") * 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text = train_df[\"Tweet\"].values.tolist()\n",
    "train_label = train_df[\"label\"].values.tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_text, train_label, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import RobertaTokenizer\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base', add_prefix_space=True, use_fast=False)\n",
    "\n",
    "#x = tokenizer(\"Lego toys, Lego cartoons, Lego designers really like kids all over the world. See a lot of cartoons about cars and Lego cartoons, cartoons about Lego police cars, cartoons about Lego trains, cartoons about Lego fire trucks on our channel.\", truncation=True, padding=True, add_special_tokens = True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\Esraa/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\ff46155979338ff8063cdad90908b498ab91b181\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Esraa/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\ff46155979338ff8063cdad90908b498ab91b181\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Esraa/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\ff46155979338ff8063cdad90908b498ab91b181\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', add_prefix_space=True, use_fast=False)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, add_special_tokens = True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, add_special_tokens = True)\n",
    "\n",
    "for sent in test_text: \n",
    "    print(sent)\n",
    "    test_encodings = tokenizer(sent, truncation=True, padding=True, add_special_tokens = True, is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_torch_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=[]):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if len(self.labels):\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx]).to(torch.int64)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = make_torch_dataset(train_encodings, train_labels)\n",
    "val_dataset = make_torch_dataset(val_encodings, val_labels)\n",
    "#test_dataset = make_torch_dataset(test_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    eval_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=50,                # number of warmup steps for learning rate scheduler\n",
    "    save_steps=100,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=20,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deca-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e49b19f4795d8aa2b46258d081b41fb60f494b9e430959b1c60548bcfc5d77f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
